---
title: "Final Project"
author: "Rachael Cooper, Sarah Gould, Nathan Patton, Samarth Saxena"
date: "11/29/2021"
output: 
  html_document:
    toc: TRUE
    toc_depth: 5
    toc_float: TRUE
    tod_collapsed: TRUE
    theme: cosmo
  editor_options:
    chunk_output_type: console
    
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
# cache --> only have to recompile the changes
# eval --> runs the code
# echo --> display the code
```

```{r, include=FALSE}
library(tidyverse)
library(ggplot2)
library(knitr)
library(plotly)
library(DT)
library(tidyverse)
library(randomForest)
library(rio)
library(mltools)
library(data.table)
library(caret)
library(pROC)
library(RColorBrewer)
```

### Introduction

Many colleges want to optimize the money they receive from their alumni. In order to do so, they need to identify and predict the salary/unemployment rate of recent graduates based on their education and other various factors. Doing so, they will be able to put more money into those programs to get a larger return on their investments (students).

*Business Question:*

Where can colleges put money in order to optimize the amount of money they receive from recent graduates?

*Analysis Question:*

Based on recent graduates and their characteristics/education, what would be their predicted median salary? Would they make over or less than six figures?

### Background Information

This data is pulled from the 2012-12 American Community Survey Public Use Microdata Series, and is limited to those users under the age of 28. The general purpose of this code and data is based upon this [story](https://fivethirtyeight.com/features/the-economic-guide-to-picking-a-college-major/). This story describes the dilemma among college students about choosing the right major, considering the financial benefits of the field and the its maximized chance to graduate. It breaks down the overarching majors like "Engineering" and "STEM," and dives deeper into what each major means in terms of later financial stability and its popularity in comparison to other majors. The actual dataset contains a detailed breakdown about basic earnings as well as labor force information, taking into account sex and the type of job acquired post graduation.

### Process Overview

1) Load & Clean Data
    a) Classify Variables Correctly
    b) One-Hot Encoding
2) Exploratory Data Analysis
3) Data Visualization 
4) Build a Linear Regression Model
    a) Prediction
    b) Evaluation
5) Build a Random Forest Model
    a) Calculate mtry Level
    b) Optimize/Tune the Model
    c) Evaluation
6) Fairness Assessment

### Data Cleaning

A brief look at the raw data can be found below.

```{r, echo = FALSE}
# Import the raw data
majors_raw <- read.csv("./recent-grads.csv")
DT::datatable(majors_raw)
# Remove rows with missing data
majors_raw <- na.omit(majors_raw)
```

```{r, echo=FALSE}
str(majors_raw)
```

As can be seen above, many of the categories are integer values. Many of these variables can be converted into factor variables in addition to the numerical ones. In addition, the variables Rank, Major Code, and Major can be dropped as the Rank variable highly correlates with the salary variable, and the other two are to specific and cannot be generalized.

```{r}
majors_added_categorical <- majors_raw %>% mutate(Over.50K = ifelse(Median > 50000, "Over", "Under.Equal"), High.Unemployment = ifelse(Unemployment_rate > 0.5, "High", "Low")) %>% select(-1, -2, -3)
```

In addition, the categorical variable categories can be compressed in order for more useful data for the analysis.

```{r, include=FALSE}
# Changing Categorical Variables into Factor Variables
majors_factors <- majors_added_categorical %>% mutate_if(sapply(majors_added_categorical, is.character), as.factor)
# Collapsing Factors
majors <- majors_factors$Major_category <- fct_collapse(majors_factors$Major_category,
                             Arts = c("Arts", "Humanities & Liberal Arts", "Industrial Arts & Consumer Services"),
                             Sciences = c("Agriculture & Natural Resources", "Biology & Life Science", "Health", "Physical Sciences", "Social Science"),
                             STEM = c("Computers & Mathematics", "Engineering"),
                             Other = c("Interdisciplinary", "Education", "Psychology & Social Work", "Communications & Journalism", "Business", "Law & Public Policy"))
```

```{r, echo=FALSE}
table(majors_factors$Major_category)
```

In order to do some analysis, all categorical variables need to be one hot encoded, which is done below:

```{r}
# One Hot Encoded Data
majors_onehot <- one_hot(data.table(majors_factors), cols = c("Major_category", "High.Unemployment"))
# Normal Data
majors <- majors_factors
```

### Exploratory Data Analysis

Before beginning with the analytical part of the exploration, it is beneficial to visualize and summarize the data in order to get a better understanding of the data in its entirety, and with an emphasis on variables you believe to be important for your analysis. 

```{r, echo=FALSE}
# Summary of Median variable
summary(majors$Median)
```

```{r, include=FALSE}
# Correlation Matrix -> Which variables have the highest correlation with one another
dat <- majors %>% select(-4, -19, -20)
dat <- as_tibble(dat, na.rm = TRUE)

# creating the correlation matrix
res <- cor(dat, use = "complete.obs")
round(res, 3)
```

```{r, echo=FALSE}
head(res, 3)
```

The above confusion matrix details the correlation coefficients between all the respective variables with "Total," "Men," and "Women." The correlation coefficient is a measure of the relationship strength between two different variables, with the magnitude closest to 1 or -1 indicating there is a strong direct and/or indirect relationship. Based on the output, it is important to note the differences among the "Employed" between men and women. Comparatively, there is a stronger direct relationship between women being employed (~0.945) when compared to men (~0.878). Similarly, women are more prone to work part-time (~0.917) when compared to men (~0.894). On the other hand, when comparing the median variable, which describes the median earnings of full-time year-round workers, women tend to have a slight inverse relationship (~ -0.182) whereas men have a slight direct relationship (~0.025). This is an important dissimilarity, considering women are slightly more employed yet do not payed as much in comparison

### Data Vizualization

Now, we can visualize the dataset. To do this, we used the ggplot and plot_ly packages. 

```{r, echo=FALSE}
# Polar Chart to understand the sampling (how many in each category)
# This will be important when connecting the results back to a real world application
ggplot(majors, 
       aes(x = Major_category)) + 
  geom_bar(fill = brewer.pal(4, "Set3")) +
  coord_polar()
```

As can be seen above, the first graph we created is a polar graph. A polar graph allows the reader to understand the sampling distribution, as well as the amount of representation each major category has in the dataset. The larger the slice, the more representation the category has in the dataset. From the polar chart, Sciences has the largest amount of representation, followed closely by the Other category. STEM is third, but by a large margin, and Arts is last.

```{r, echo=FALSE}
# Stacked Bar Graph
ggplot(majors, 
       aes(x = Major_category, 
           fill = Over.50K)) + 
  geom_bar(position = "fill")
```

The next graph we created was a stacked bar graph. The major category is on the x-axis, while the count - normalized to be between 0 and 1 - is on the y-axis. The fill of the graph is based on whether or not a person from that category has a median salary that is larger than $50,000. From this graph, it seems that STEM majors have almost 50 percent of their category making above 50K per year - the largest percentage of the four major categories. The other three major categories are nowhere close to STEM, with the Other category coming in second with about 7 percent of their category making above 50K. Science is third with what seems to be about 1 percent of their category making above 50K, and Art is last with what seems to be 0 percent of their category making above 50K per year.


```{r, echo=FALSE}
# Box Plots
ggplot(majors, aes(x = Major_category, y = Median)) +
  geom_boxplot(fill = brewer.pal(4, "Set3"),
               color = "turquoise",
               outlier.color = "pink") +
   coord_flip()
```

For our third graph, we decided to make a box-plot graph where the x-axis is the median salary and the y-axis is the four major categories. From this graph, it can be deduced that the range of STEM majors is higher than that of any other major. The range of STEM majors seems to be about 40-50K, whereas the other majors have a maximum range of 30K. There is a STEM major who currently has a median salary of 120K, which is almost double the highest median salary of any other major category. Another interesting aspect about the STEM box-plot, when compared to the other three, is that the median salary for the 25th percentile of STEM is equal to about 45K, which is higher than the median salary of the 75th percentile for any other category. The other three boxplots are relatively similar to each other, with the Art category being much skinnier than the other two. The skinnier the graph, the smaller the range of the graph.

```{r, echo=FALSE}
# Pre-analysis Data Visualization

# Use plotly to do a 3d imaging ~ is unique to plotly & means use all the layers
fig <- plot_ly(majors, 
               type = "scatter3d",
               mode="markers",
               symbol = "circle",
               x = ~Unemployment_rate, 
               y = ~ShareWomen,
               z = ~Low_wage_jobs,
               marker = list(color = ~Median, colorscale = "Viridis", showscale = TRUE), 
               # hover text (shows up when we hover over a point)
               text = ~paste('Salary Median:', Median,
                             'unemployement Rate:', Unemployment_rate))

fig
# dev.off()

```

Our final graph above is a three-dimensional scatterplot. The unemployment rate on a scale of 0-1 is on the x-axis, the share of women as a decimal is on the y-axis, and the z-axis has the salary of the low-wage jobs that people work. The color of the marker is dependent on how much money each person makes, and uses a gradient color scheme. From the graph, it can be determined that women do not make as much money as men as they are working in lower-wage jobs and have higher rates of unemployment. Another interesting thing to note is that only one student overall made above a 100K median salary.

### Model Building Linear Regression

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Build Linear Regression Model for Median category
median_dt <- majors_onehot[,-c("Median")]
# view(median_dt)
part_index_1 <- caret::createDataPartition(median_dt$Over.50K,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)
# View(part_index_1)
dim(median_dt)
train <- median_dt[part_index_1,]
tune_and_test <- median_dt[-part_index_1, ]
#The we need to use the function again to create the tuning set 
tune_and_test_index <- createDataPartition(tune_and_test$Over.50K,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)
tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]
dim(train)
dim(tune)
dim(test)
#Cross validation process 
fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5, 
                          returnResamp="all",
                          savePredictions = 'final') 
# number - number of folds
# repeats - number of times the CV is repeated, here it's 5 take the average of
# those 5 repeats
#returnResamp - essential verbose, save all of the sampling data
features <- train[,-"Over.50K"]
target <- train$Over.50K
# View(target)
str(features)
set.seed(1984)
median_mdl <- train(x=features,
                    y=target,
                    trControl=fitControl,
                    method="C5.0",
                    verbose=TRUE)
```


```{r}
median_mdl
```

Based on the accuracies and the respective kappa values for each set of trials, both with and without winnowing, the final model was chosen with 1 trial and without winnowing. This indicates that the lowest possible number of boosting iterations is favorable. The accuracy for winnowing with 20 trials was approximately 0.8834 and comparatively the accuracy for no winnowing with 1 trials was approximately 0.9295. Based on those values, the difference between the number of trials along with factoring in no winnowing or winnowing was a good amount; therefore, it is important to highlight that no winnowing and a small trial number would be the most suitable option for constructing this model.

```{r}
# plot the model
plot(median_mdl)
```

Graphically, the difference between no winnowing (FALSE) and winnowing (TRUE) and the respective trials can be visualized. As seen in the graph, the graph trends downwards as the number of trials increased. The FALSE graph is also much higher relating to accuracy, when compared to TRUE. This visualization can be supported by the previous output that identified 1 trial and no winnowing, as the most favorable final model.

#### Prediction 

```{r, echo=FALSE}
# Predict Linear Regression Model
median_predict = predict(median_mdl,tune,type= "raw")
confusionMatrix(as.factor(median_predict), 
                as.factor(tune$Over.50K), 
                dnn=c("Prediction", "Actual"), 
                mode = "everything")
```

From the generated confusion matrix, the most useful metrics for analysis would be the accuracy coupled with the F1 score. The goal is to have accuracy be as close to 1 as possible; therefore, with that understanding, the value of *0.9615* is very optimal. It could continue to be optimized to have the value be closer to 1, but that value would be considered "great" in terms of the model as a whole. The F1 score is a measure of the model's accuracy. The value of *0.8571* from the statistics indicates that the model is pretty accurate but similar to the accuracy, it should also be improved in order to be as close to 1 as possible.

#### Evaluation 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Evaluate the linear regression model
varImp(median_mdl)
grid <- expand.grid(.winnow = c(TRUE,FALSE), 
                    .trials=c(20,30,40), 
                    .model=c("tree","rules"))
set.seed(1984)
median_mdl_tune <- train(x=features,
                y=target,
                tuneGrid=grid,
                trControl=fitControl,
                method="C5.0",
                verbose=TRUE)
median_mdl_tune
median_mdl
# Want to evaluation again with the tune data using the new model 
median_predict_tune = predict(median_mdl_tune,tune,type= "raw")
confusionMatrix(as.factor(median_predict_tune), 
                as.factor(tune$Over.50K), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec")
#Test
median_predict_test = predict(median_mdl_tune,test,type = "raw")
confusionMatrix(as.factor(median_predict_test),
                as.factor(test$Over.50K),
                dnn=c("Prediction","Actual"),
                mode = "sens_spec")
```

### Model Building Classification Random Forest

```{r, include=FALSE}
# Create combined target variable with (inverse of unemployment * median) categories
combined_target <- majors$Median * (1 - majors$Unemployment_rate) * majors$ShareWomen
majors_combined_target <- data.frame(majors, combined_target)

# view(majors_combined_target)
```

```{r, include=FALSE}
# Next let's one-hot encode those factor variables/character 
majors_combined_target$combined_target <-ifelse(majors_combined_target$combined_target > 20000,1,0)

#added this a predictor versus replacing the numeric version
(majors_combined_target$combined_target <- cut(majors_combined_target$combined_target,c(-1,0.3953488,1),labels = c(0,1)))
majors_combined_target$combined_target <- fct_collapse(majors_combined_target$combined_target, "LE.EQ.20K"="0", "G.50K"="1")
majors_combined_target <- majors_combined_target %>%
  mutate(combined_target = factor(combined_target, labels = make.names(levels(combined_target))))

str(majors_combined_target)
```

```{r, echo=FALSE}
#Determine the baserate or prevalence for the classifier

(prevalence <- table(majors_combined_target$combined_target)[[2]]/length(majors_combined_target$combined_target))
table(majors_combined_target$combined_target)
```

```{r, echo=FALSE}
# Split data into Train, Tune, Test
part_index_1 <- caret::createDataPartition(majors_combined_target$combined_target,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)
train <- majors_combined_target[part_index_1, ]
tune_and_test <- majors_combined_target[-part_index_1, ]
#The we need to use the function again to create the tuning set 
tune_and_test_index <- createDataPartition(tune_and_test$combined_target,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)
tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]
dim(train)
dim(test)
dim(tune)
# these are slightly off because the data set isn't perfectly even
```

```{r, echo=FALSE}
#Calculate the initial mtry level 
mytry_tune <- function(x){
  y <- dim(x)[2]-1
  sqrt(y)
}

mytry_tune(majors_combined_target)
```

```{r, include=FALSE}
#Creating an initial random forest model with 500 trees
set.seed(2023)
combined_RF = randomForest(combined_target~.,          #<- Formula: response variable ~ predictors.
                            #   The period means 'use all other variables in the data'.
                            train,     #<- A data frame with the variables to be used.
                            #y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
                            #subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
                            #xtest = NULL,       #<- This is already defined in the formula by the ".".
                            #ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
                            ntree = 500,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
                            mtry = 4,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
                            replace = TRUE,      #<- Should sampled data points be replaced.
                            #classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population 
                            #strata = NULL,      #<- Not necessary for our purpose here.
                            sampsize = 100,      #<- Size of sample to draw each time.
                            nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
                            #maxnodes = NULL,    #<- Limits the number of maximum splits. 
                            importance = TRUE,   #<- Should importance of predictors be assessed?
                            #localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
                            proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
                            norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
                            do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
                            keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
                            keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees? 

# Look at the output of the random forest.
combined_RF
```

```{r, echo=FALSE}
# Determining the number of trees that should be used 

# The "err.rate" argument includes a list of the cumulative error rates
# for each tree, by class and in aggregate for data points not 
# included in the tree (OOB).
# View(as.data.frame(combined_RF$err.rate))

err.rate <- as.data.frame(combined_RF$err.rate)

# View(err.rate)

# The "oob.times" argument includes the number of times that each data point
# is not excluded from trees in the random forest.

# View(as.data.frame(combined_RF$oob.times))

combined_RF_error = data.frame(1:nrow(combined_RF$err.rate),
                                combined_RF$err.rate)
head(combined_RF_error, 10)

colnames(combined_RF_error) = c("Number of Trees", "Out of the Box","<=20K", ">20K")

combined_RF_error$Diff <- combined_RF_error$'>20K'-combined_RF_error$`<=20K`

# View(combined_RF_error)

# 54 Trees should be used because that amount is correlated to the minimum OOB error and >20K value. 
```

```{r, echo=FALSE}
#Determining the right number of variables to randomly sample (the mtry parameter)

str(train)
set.seed(2)
combined_RF_mtry = tuneRF(data.frame(train[ ,1:20]),  #<- data frame of predictor variables
                           (train[ ,21]),              #<- response vector (variables), factors for classification and continuous variable for regression
                           mtryStart = 4,                        #<- starting value of mtry, the default is the same as in the randomForest function
                           ntreeTry = 79,                        #<- number of trees used at the tuning step, let's use the same number as we did for the random forest
                           stepFactor = 2,                       #<- at each iteration, mtry is inflated (or deflated) by this value
                           improve = 0.05,                       #<- the improvement in OOB error must be by this much for the search to continue
                           trace = TRUE,                         #<- whether to print the progress of the search
                           plot = TRUE,                          #<- whether to plot the OOB error as a function of mtry
                           doBest = TRUE)                       #<- whether to create a random forest using the optimal mtry parameter

combined_RF_mtry

#Based on the output of the combined_RF_mtry, it looks like 20 variables is the right number of variables to sample becauses it has the least OOB error compared to 2,4,8, and 16. 
```


```{r, include=FALSE}
# Build Random Forest Classification Model for Combined Category in consideration of the number of trees, the number of variables to sample and the sample size that optimize the model output. 
set.seed(2023)
combined_RF_2 = randomForest(combined_target~.,          #<- Formula: response variable ~ predictors.
                            #   The period means 'use all other variables in the data'.
                            train,     #<- A data frame with the variables to be used.
                            #y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
                            #subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
                            #xtest = NULL,       #<- This is already defined in the formula by the ".".
                            #ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
                            ntree = 54,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
                            mtry = 20,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
                            replace = TRUE,      #<- Should sampled data points be replaced.
                            #classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population 
                            #strata = NULL,      #<- Not necessary for our purpose here.
                            sampsize = 100,      #<- Size of sample to draw each time.
                            nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
                            #maxnodes = NULL,    #<- Limits the number of maximum splits. 
                            importance = TRUE,   #<- Should importance of predictors be assessed?
                            #localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
                            proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
                            norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
                            do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
                            keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
                            keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees? 

# Look at the output of the random forest.
combined_RF_2

#The sample size of the model was kept at the original value of 100, because it was found that this value minimized the class error as much as possible for both classes. When the sample size was increased or decreased, one of the class errors tends to fall, but the other rises significantly. Therefore, this is the best sample size that will minimize class errors, and prevent over or under fitting of the data.
```

#### Tuning

Because the built in Random Forest Model was not agreeable with the tuning done with the caret library, an original random forest classification tuning metric was created in order to determine the best values for the three hyperparameters determined above.

```{r, echo=FALSE}
# Tune the model
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree", "sampsize"), class = rep("numeric", 3), label = c("mtry", "ntree", "sampsize"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
```

Now, we can set the hyperparameter values to try and tune the model.

```{r, echo=FALSE}
# Build your model using the training data and default settings in caret, double check to make sure you are using a cross-validation training approach
set.seed(1)
# Choose the features and classes 
# DROP the target variable
features <- train[,-19]
target <- train$Over.50K
# Cross validation process
# trainControl -> how do you want to train
fitControl <- trainControl(method = "repeatedcv",
                          number = 5, # number of folds : k = 10
                          repeats = 5, # the number of complete sets of folds to compute (repeat 5 times -> total 50 models)
                          search = "grid",
                          returnResamp="all", # return a list of all samples
                          classProbs = TRUE, # return the probabilities
                          allowParallel = TRUE,
                          summaryFunction = twoClassSummary) # return the models in parallel -> more efficient
# number - number of folds
# repeats - number of times the CV is repeated, here it's 5 take the average of those 5 repeats
grid <- expand.grid(.mtry = c(3, 4, 5),
                    .sampsize = c(50, 100, 200),
                    .ntree = c(200, 300, 400) # try different number of boosting iterations
                    #.model="rf")
)
# expand.grid - series of options that are available for model training based on different set hyper parameters
grid
# Specific Hyper Parameters of C50:
# winnow - whether to reduce the feature space - works to remove unimportant features but it doesn't always work
# trails - number of boosting iterations to try, 1 indicates a single model
# model - type of ml model
# train

# Need to drop high unemployement because it is a constant in the training set
train2 <- train %>% select(-High.Unemployment)
mdl <- train(Over.50K~., 
             data = train2,
             method=customRF,
             metric='ROC', 
             tuneGrid=grid,
             trControl=fitControl,
             verbose=TRUE)
mdl
```
#### Evaluation 

```{r}
# Evaluation of Model
```

### Fairness Assesment

### Conclusion

What can you say about the results of the methods section as it relates to your question given the limitations to your model?

### Future Recommendations

What additional analysis is needed or what limited your analysis on this project?
One additional piece of analysis that would benefit the report as a whole is using recently recorded data. The data that was used in this analysis was recorded from 2010-2012, so the trends that were discovered from our analysis are most likely outdated. Having new data would greatly benefit the university that wanted this report, as they would be able to adjust major categories based on newer trends rather than older ones. Another additional piece of analysis that would benefit our report would be the addition of the decision tree model. In our analysis, we included linear regression and the random forest model. However, we never include the decision tree model, which would have allowed us to see a model where the most optimal choice was made every time - since the decision tree is a greedy algorithm by nature. Including the decision tree would have made our analysis more diverse and well-rounded, as we would have had performed an analysis using three different major analytic methods. Personally, we don't believe that anything limited our analysis on the project - the dataset was easy to work with and the models that we created learned the data efficiently and effectively. 
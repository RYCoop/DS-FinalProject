---
title: "Final Project"
author: "Rachael Cooper, Sarah Gould, Nathan Patton, Samarth Saxena"
date: "11/29/2021"
output: 
  html_document:
    toc: TRUE
    toc_depth: 5
    toc_float: TRUE
    tod_collapsed: TRUE
    theme: cosmo
  editor_options:
    chunk_output_type: console
    
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
# cache --> only have to recompile the changes
# eval --> runs the code
# echo --> display the code
```

```{r, include=FALSE}
library(tidyverse)
library(ggplot2)
library(knitr)
library(plotly)
library(DT)
library(tidyverse)
library(randomForest)
library(rio)
library(mltools)
library(data.table)
library(caret)
library(pROC)
library(RColorBrewer)
```

### Introduction

Many colleges want to optimize the money they receive from their alumni. In order to do so, they need to identify and predict the salary/unemployment rate of recent graduates based on their education and other various factors. Doing so, they will be able to put more money into those programs to get a larger return on their investments (students).

*Business Question:*

Where can colleges put money in order to optimize the amount of money they receive from recent graduates?

*Analysis Question:*

Based on recent graduates and their characteristics/education, what would be their predicted median salary? Would they make over or less than six figures?

### Background Information

What about the data do we need to know...

### Process Overview

What will we be doing? Methods, techniques, why?

### Data Cleaning

A brief look at the raw data can be found below.

```{r, echo = FALSE}
# Import the raw data
majors_raw <- read.csv("./recent-grads.csv")
DT::datatable(majors_raw)
# Remove rows with missing data
majors_raw <- na.omit(majors_raw)
```

```{r}
str(majors_raw)
```

As can be seen above, many of the categories are integer values. Many of these variables can be converted into factor variables in addition to the numerical ones. In addition, the variables Rank, Major Code, and Major can be dropped as the Rank variable highly correlates with the salary variable, and the other two are to specific and cannot be generalized.

```{r}
majors_added_categorical <- majors_raw %>% mutate(Over.50K = ifelse(Median > 50000, "Over", "Under.Equal"), High.Unemployment = ifelse(Unemployment_rate > 0.5, "High", "Low")) %>% select(-1, -2, -3)
```

In addition, the categorical variable categories can be compressed in order for more useful data for the analysis.

```{r}
# Changing Categorical Variables into Factor Variables
majors_factors <- majors_added_categorical %>% mutate_if(sapply(majors_added_categorical, is.character), as.factor)
# Collapsing Factors
majors <- majors_factors$Major_category <- fct_collapse(majors_factors$Major_category,
                             Arts = c("Arts", "Humanities & Liberal Arts", "Industrial Arts & Consumer Services"),
                             Sciences = c("Agriculture & Natural Resources", "Biology & Life Science", "Health", "Physical Sciences", "Social Science"),
                             STEM = c("Computers & Mathematics", "Engineering"),
                             Other = c("Interdisciplinary", "Education", "Psychology & Social Work", "Communications & Journalism", "Business", "Law & Public Policy"))
```

```{r}
table(majors_factors$Major_category)
```

In order to do some analysis, all categorical variables need to be one hot encoded, which is done below:

```{r}
# One Hot Encoded Data
majors_onehot <- one_hot(data.table(majors_factors), cols = c("Major_category", "High.Unemployment"))
# Normal Data
majors <- majors_factors
```

### Exploratory Data Analysis

Before beginning with the analytical part of the exploration, it is beneficial to visualize and summarize the data in order to get a better understanding of the data in its entirety, and with an emphasis on variables you believe to be important for your analysis. 

```{r}
# Summary of Median variable
summary(majors$Median)
```

```{r}
# Correlation Matrix -> Which variables have the highest correlation with one another
dat <- majors %>% select(-4, -19, -20)
dat <- as_tibble(dat, na.rm = TRUE)

# creating the correlation matrix
res <- cor(dat, use = "complete.obs")
round(res, 3)
```

### Data Vizualization

```{r}
# Polar Chart to understand the sampling (how many in each category)
# This will be important when connecting the results back to a real world application
ggplot(majors, 
       aes(x = Major_category)) + 
  geom_bar(fill = brewer.pal(4, "Set3")) +
  coord_polar()
```

```{r}
# Stacked Bar Graph
ggplot(majors, 
       aes(x = Major_category, 
           fill = Over.50K)) + 
  geom_bar(position = "fill")
```

```{r}
# Box Plots
ggplot(majors, aes(x = Major_category, y = Median)) +
  geom_boxplot(fill = brewer.pal(4, "Set3"),
               color = "turquoise",
               outlier.color = "pink") +
   coord_flip()
```

```{r}
# Pre-analysis Data Visualization

# Use plotly to do a 3d imaging ~ is unique to plotly & means use all the layers
fig <- plot_ly(majors, 
               type = "scatter3d",
               mode="markers",
               symbol = "circle",
               x = ~Unemployment_rate, 
               y = ~ShareWomen,
               z = ~Low_wage_jobs,
               marker = list(color = ~Median, colorscale = "Viridis", showscale = TRUE), 
               # hover text (shows up when we hover over a point)
               text = ~paste('Salary Median:', Median,
                             'unemployement Rate:', Unemployment_rate))

fig
# dev.off()

```

### Model Building Linear Regression

```{r}
# Build Linear Regression Model for Median category
median_dt <- majors_onehot[,-c("Median")]
# view(median_dt)
part_index_1 <- caret::createDataPartition(median_dt$Over.50K,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)
# View(part_index_1)
dim(median_dt)
train <- median_dt[part_index_1,]
tune_and_test <- median_dt[-part_index_1, ]
#The we need to use the function again to create the tuning set 
tune_and_test_index <- createDataPartition(tune_and_test$Over.50K,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)
tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]
dim(train)
dim(tune)
dim(test)
#Cross validation process 
fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5, 
                          returnResamp="all",
                          savePredictions = 'final') 
# number - number of folds
# repeats - number of times the CV is repeated, here it's 5 take the average of
# those 5 repeats
#returnResamp - essential verbose, save all of the sampling data
features <- train[,-"Over.50K"]
target <- train$Over.50K
# View(target)
str(features)
set.seed(1984)
median_mdl <- train(x=features,
                    y=target,
                    trControl=fitControl,
                    method="C5.0",
                    verbose=TRUE)
median_mdl
```

#### Prediction 

```{r}
# Predict Linear Regression Model
median_predict = predict(median_mdl,tune,type= "raw")
confusionMatrix(as.factor(median_predict), 
                as.factor(tune$Over.50K), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec")
```

```{r}
# Given a certain values for the other variables predict the Median Salary
```

#### Evaluation 

```{r}
# Evaluate the linear regression model
varImp(median_mdl)
grid <- expand.grid(.winnow = c(TRUE,FALSE), 
                    .trials=c(20,30,40), 
                    .model=c("tree","rules"))
set.seed(1984)
median_mdl_tune <- train(x=features,
                y=target,
                tuneGrid=grid,
                trControl=fitControl,
                method="C5.0",
                verbose=TRUE)
median_mdl_tune
median_mdl
# Want to evaluation again with the tune data using the new model 
median_predict_tune = predict(median_mdl_tune,tune,type= "raw")
confusionMatrix(as.factor(median_predict_tune), 
                as.factor(tune$Over.50K), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec")
#Test
median_predict_test = predict(median_mdl_tune,test,type = "raw")
confusionMatrix(as.factor(median_predict_test),
                as.factor(test$Over.50K),
                dnn=c("Prediction","Actual"),
                mode = "sens_spec")
```

### Model Building Classification Random Forest

```{r}
# Create combined target variable with (inverse of unemployment * median) categories
combined_target <- majors$Median * (1 - majors$Unemployment_rate) * majors$ShareWomen
majors_combined_target <- data.frame(majors, combined_target)

# view(majors_combined_target)
```

```{r}
# Next let's one-hot encode those factor variables/character 
majors_combined_target$combined_target <-ifelse(majors_combined_target$combined_target > 20000,1,0)

#added this a predictor versus replacing the numeric version
(majors_combined_target$combined_target <- cut(majors_combined_target$combined_target,c(-1,0.3953488,1),labels = c(0,1)))
majors_combined_target$combined_target <- fct_collapse(majors_combined_target$combined_target, "LE.EQ.20K"="0", "G.50K"="1")
majors_combined_target <- majors_combined_target %>%
  mutate(combined_target = factor(combined_target, labels = make.names(levels(combined_target))))

str(majors_combined_target)
```

```{r}
#Determine the baserate or prevalence for the classifier

(prevalence <- table(majors_combined_target$combined_target)[[2]]/length(majors_combined_target$combined_target))
table(majors_combined_target$combined_target)
```

```{r}
# Split data into Train, Tune, Test
part_index_1 <- caret::createDataPartition(majors_combined_target$combined_target,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)
train <- majors_combined_target[part_index_1, ]
tune_and_test <- majors_combined_target[-part_index_1, ]
#The we need to use the function again to create the tuning set 
tune_and_test_index <- createDataPartition(tune_and_test$combined_target,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)
tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]
dim(train)
dim(test)
dim(tune)
# these are slightly off because the data set isn't perfectly even
```

```{r}
#Calculate the initial mtry level 
mytry_tune <- function(x){
  y <- dim(x)[2]-1
  sqrt(y)
}

mytry_tune(majors_combined_target)
```

```{r}
#Creating an initial random forest model with 500 trees
set.seed(2023)
combined_RF = randomForest(combined_target~.,          #<- Formula: response variable ~ predictors.
                            #   The period means 'use all other variables in the data'.
                            train,     #<- A data frame with the variables to be used.
                            #y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
                            #subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
                            #xtest = NULL,       #<- This is already defined in the formula by the ".".
                            #ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
                            ntree = 500,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
                            mtry = 4,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
                            replace = TRUE,      #<- Should sampled data points be replaced.
                            #classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population 
                            #strata = NULL,      #<- Not necessary for our purpose here.
                            sampsize = 100,      #<- Size of sample to draw each time.
                            nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
                            #maxnodes = NULL,    #<- Limits the number of maximum splits. 
                            importance = TRUE,   #<- Should importance of predictors be assessed?
                            #localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
                            proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
                            norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
                            do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
                            keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
                            keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees? 

# Look at the output of the random forest.
combined_RF
```

```{r}
# Determining the number of trees that should be used 

# The "err.rate" argument includes a list of the cumulative error rates
# for each tree, by class and in aggregate for data points not 
# included in the tree (OOB).
# View(as.data.frame(combined_RF$err.rate))

err.rate <- as.data.frame(combined_RF$err.rate)

# View(err.rate)

# The "oob.times" argument includes the number of times that each data point
# is not excluded from trees in the random forest.

# View(as.data.frame(combined_RF$oob.times))

combined_RF_error = data.frame(1:nrow(combined_RF$err.rate),
                                combined_RF$err.rate)
combined_RF_error

colnames(combined_RF_error) = c("Number of Trees", "Out of the Box","<=20K", ">20K")

combined_RF_error$Diff <- combined_RF_error$'>20K'-combined_RF_error$`<=20K`

# View(combined_RF_error)

# 54 Trees should be used because that amount is correlated to the minimum OOB error and >20K value. 
```

```{r}
#Determining the right number of variables to randomly sample (the mtry parameter)

str(train)
set.seed(2)
combined_RF_mtry = tuneRF(data.frame(train[ ,1:20]),  #<- data frame of predictor variables
                           (train[ ,21]),              #<- response vector (variables), factors for classification and continuous variable for regression
                           mtryStart = 4,                        #<- starting value of mtry, the default is the same as in the randomForest function
                           ntreeTry = 79,                        #<- number of trees used at the tuning step, let's use the same number as we did for the random forest
                           stepFactor = 2,                       #<- at each iteration, mtry is inflated (or deflated) by this value
                           improve = 0.05,                       #<- the improvement in OOB error must be by this much for the search to continue
                           trace = TRUE,                         #<- whether to print the progress of the search
                           plot = TRUE,                          #<- whether to plot the OOB error as a function of mtry
                           doBest = TRUE)                       #<- whether to create a random forest using the optimal mtry parameter

combined_RF_mtry

#Based on the output of the combined_RF_mtry, it looks like 20 variables is the right number of variables to sample becauses it has the least OOB error compared to 2,4,8, and 16. 
```


```{r}
# Build Random Forest Classification Model for Combined Category in consideration of the number of trees, the number of variables to sample and the sample size that optimize the model output. 
set.seed(2023)
combined_RF_2 = randomForest(combined_target~.,          #<- Formula: response variable ~ predictors.
                            #   The period means 'use all other variables in the data'.
                            train,     #<- A data frame with the variables to be used.
                            #y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
                            #subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
                            #xtest = NULL,       #<- This is already defined in the formula by the ".".
                            #ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
                            ntree = 54,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
                            mtry = 20,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
                            replace = TRUE,      #<- Should sampled data points be replaced.
                            #classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population 
                            #strata = NULL,      #<- Not necessary for our purpose here.
                            sampsize = 100,      #<- Size of sample to draw each time.
                            nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
                            #maxnodes = NULL,    #<- Limits the number of maximum splits. 
                            importance = TRUE,   #<- Should importance of predictors be assessed?
                            #localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
                            proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
                            norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
                            do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
                            keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
                            keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees? 

# Look at the output of the random forest.
combined_RF_2

#The sample size of the model was kept at the original value of 100, because it was found that this value minimized the class error as much as possible for both classes. When the sample size was increased or decreased, one of the class errors tends to fall, but the other rises significantly. Therefore, this is the best sample size that will minimize class errors, and prevent over or under fitting of the data.
```

#### Tuning

Because the built in Random Forest Model was not agreeable with the tuning done with the caret library, an original random forest classification tuning metric was created in order to determine the best values for the three hyperparameters determined above.

```{r}
# Tune the model
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree", "sampsize"), class = rep("numeric", 3), label = c("mtry", "ntree", "sampsize"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
    randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
```

Now, we can set the hyperparameter values to try and tune the model.

```{r, echo=FALSE}
# Build your model using the training data and default settings in caret, double check to make sure you are using a cross-validation training approach
set.seed(1)
# Choose the features and classes 
# DROP the target variable
features <- train[,-19]
target <- train$Over.50K
# Cross validation process
# trainControl -> how do you want to train
fitControl <- trainControl(method = "repeatedcv",
                          number = 5, # number of folds : k = 10
                          repeats = 5, # the number of complete sets of folds to compute (repeat 5 times -> total 50 models)
                          search = "grid",
                          returnResamp="all", # return a list of all samples
                          classProbs = TRUE, # return the probabilities
                          allowParallel = TRUE,
                          summaryFunction = twoClassSummary) # return the models in parallel -> more efficient
# number - number of folds
# repeats - number of times the CV is repeated, here it's 5 take the average of those 5 repeats
grid <- expand.grid(.mtry = c(3, 4, 5),
                    .sampsize = c(50, 100, 200),
                    .ntree = c(200, 300, 400) # try different number of boosting iterations
                    #.model="rf")
)
# expand.grid - series of options that are available for model training based on different set hyper parameters
grid
# Specific Hyper Parameters of C50:
# winnow - whether to reduce the feature space - works to remove unimportant features but it doesn't always work
# trails - number of boosting iterations to try, 1 indicates a single model
# model - type of ml model
# train

# Need to drop high unemployement because it is a constant in the training set
train2 <- train %>% select(-High.Unemployment)
mdl <- train(Over.50K~., 
             data = train2,
             method=customRF,
             metric='ROC', 
             tuneGrid=grid,
             trControl=fitControl,
             verbose=TRUE)
mdl
```
#### Evaluation 

```{r}
# Evaluation of Model
```

### Fairness Assesment

### Conclusion

What can you say about the results of the methods section as it relates to your question given the limitations to your model?

### Future Recommendations

What additional analysis is needed or what limited your analysis on this project?
